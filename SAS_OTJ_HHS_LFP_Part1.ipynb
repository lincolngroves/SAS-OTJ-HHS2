{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# SAS On-the-Job | Part 1 \n",
    "## Role: Data Analyst within the Department of Health and Human Services"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Starting with a Story\n",
    "The coronavirus outbreak, or COVID-19, simultaneously created a health and economic crisis in the United States. As local regions locked down to slow the spread of the disease, millions of jobs were lost across the United States. Most schools also shuttered their doors and switched to remote learning. This meant that many working parents who retained their jobs found themselves as both remote workers and teacher’s assistants during the lockdowns. <br>\n",
    "\n",
    "The impacts of COVID-19 have been very uneven, especially for mothers. A focus of public policy for decades has been to find a way to get mothers back into the labor force, while supporting them as they balance work and home responsibilities. In this SAS On-the-Job activity, we explore the pre- and post-COVID-19 trends in unemployment (UE) and labor force participation (LFP) rates among U.S. women aged 25 to 54. <br>\n",
    "\n",
    "#### Job Setting \n",
    "You are a new *Public Policy Analyst* within the Department of Health and Human Services (HHS). You are placed into a department interested in public policies that promote labor force participation (LFP) by women – particularly low-income and less-educated women. Labor force participation simply means that women are engaged in the labor market - either they have a job or are looking for one.  We'll also explore the more familiar unemployment rate, which captures women who would like a job, but don't currently have one.<br>\n",
    "\n",
    "During the height of the pandemic, a host of media articles highlighted the disproportionate impact of COVID-19 on women’s employment.  HHS leadership is interested in knowing whether these impacts are still being felt today - years after the official declaration of the pandemic.  If yes, HHS leadership would consider enacting new, targeted policy response to support this potentially vulnerable group of workers. However, before considering policy responses, HHS leadership would like someone to examine whether there are lingering effects of COVID-19 on current level of labor supply by women. And this someone is you!  Drumroll, please!<br>\n",
    "\n",
    "As a new analyst, your goals in this set of exercises are to<br>\n",
    "•\taccess the Jupyter notebooks created by your predecessor <br>\n",
    "•\tunderstand what the previous analyst did - particularly with the integration of SAS, SQL, Python, and R <br>\n",
    "•\tmodify existing code to explore (as appropriate). <br>\n",
    "\n",
    "And you'll be guided in this exercise by a very helpful onboarding buddy. Which is me. Your humble narrator.\n",
    "\n",
    "Let's get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### From a technical perspective, this SAS On-the-Job provides\n",
    "Beyond allowing you to play policy analyst for the day, this SAS On-the-Job (OTJ) provides an overview of SAS, SQL, Python, and R integration in SAS Viya Workbench.  More specifically, this SAS OTJ is an act in three parts, with the following flow: <br>\n",
    "\n",
    "(1) Start in a Python notebook - because the original coder loved Python.  Perform a preliminary analysis of the data.  Then use SQL to collapse the data - because it's a much more effective way to aggregate data than other approaches in Python.  And finally, we'll ensure that the underlying data make sense. <br>\n",
    "(2) The second notebook focuses on SAS code. We'll leverage some SAS code and macros to explore aggregated U.S. trends. We'll conclude by creating a state-level data set that we can then use to map trends in R (confession: R can produce some great maps!). <br>\n",
    "(3) The last notebook focuses on R code.  We'll plot state-level trends over time for unemployment and labor force participation. <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python, SAS and SQL Integration\n",
    "### Current Population Survey (CPS) Data Import \n",
    "\n",
    "Welcome to Day 1 as Public Policy Analyst at HHS! With access to software and your data setup, we can officially begin our analysis.  Yay!\n",
    "<br> \n",
    "\n",
    "*Note: you can follow along in the notebook or – if you’re feeling really brave – recreate the notebook from the narrative below.  Either way, learning is the objective here!*<br>\n",
    "\n",
    "The first thing we’ll need to do in our data adventure is get the data loaded into the environment and running. Toward that goal, let's first connect with the SAS Viya Workbench server - where we will run our entire analysis. After making this connection, we'll then use the \"SASpy\" Python package to establish a connection between SAS and Python - and run some SAS code within the Python kernel to load the Current Population Survey Data.\n",
    "<br>\n",
    "\n",
    "And why start with SAS code in a Python program? Well, this project has used hackers from all backgrounds in the analysis - and one of the previous analysts was a big fan of using SAS to read in their data. But, know that reading in the data via Python would have been perfectly acceptable too!\n",
    "<br>\n",
    "\n",
    "The first step: ensure that saspy is installed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install saspy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-28T20:39:18.558294Z",
     "start_time": "2021-06-28T20:38:42.247615Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import saspy\n",
    "\n",
    "# Create a SAS session object\n",
    "sas_session = saspy.SASsession()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the connection established, we can now use the [`submit` method](https://sassoftware.github.io/saspy/api.html#saspy.SASsession.submit) to run SAS code using a Python kernel. Notice how we use the LIBNAME statement to save the CPS file to disk - and while pulling the original data from GitHub. Moreover, we create a Python data frame at the end, so that we can forge on with the Python portion of the analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-28T20:40:01.169277Z",
     "start_time": "2021-06-28T20:39:57.785063Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "results_dict = sas_session.submitLST(\n",
    "             \"\"\"\n",
    "                libname cps \"/workspaces/myfolder/SAS-OTJ-HHS2/SAS Data\" ;\n",
    "\n",
    "                filename git_url url 'https://raw.githubusercontent.com/lincolngroves/SAS-OTJ-HHS2/main/CPS_2015_2023_ltd.csv';\n",
    "                \n",
    "                /* Use PROC IMPORT to read the CSV file */\n",
    "                proc import datafile=git_url\n",
    "                            out=cps.cps_2015_2023\n",
    "                            dbms=csv\n",
    "                            replace;\n",
    "                            guessingrows=5000;\n",
    "                run;\n",
    "\n",
    "              \"\"\",\n",
    "             )\n",
    "\n",
    "# Convert SAS data file to Python data frame\n",
    "cps_df = sas_session.sd2df(table=\"cps_2015_2023\",libref=\"cps\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examine and Explore Our Data Frame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alright - the data are now accessible in the notebook.  What's typically next as an analyst?  Well, let's examine the underlying data to become more familiar with the data and distribution of values!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# One Simple line will pull up the Python dataframe\n",
    "cps_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the data above, we can see that we'll have nine variables in our analysis.  The current unit of analysis in this data is state, year-quarter, race-ethnic, and child-status.  In simple English, this means that a row - that is, the observation - contains the unemployment rate and labor force participation rate for women of a particular race-ethnic group, education level, and child status - for a given state at a point in time.  Yup, that's a mouthful, but still very important.  Moreover, note that **Unemp** and **in_LF** are rounded in the window above - but are not in the underlying data.  Finally, **WTFINL** is a weighting variable that represents the number of women represented in that row.  So, 8709.3981 can be interpreted as ~8709 women. This variable will be very important when we aggregate the data.\n",
    "\n",
    "Finally, **FIPS** are Federal Information Processing Standards, so the **State_Fip** is just a unique variable for that state. And it will help in the upcoming merges. Finally, **State** is just, well, a U.S. state.\n",
    "\n",
    "With data now upload in Python, let's load some Python packages so that we can do some cooler things with the data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-28T20:41:13.321932Z",
     "start_time": "2021-06-28T20:41:11.275031Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Classic Python packages that will make our Exploratory Data Analysis easier \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summarize the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's summarize the data in Python - so that we can better understand the distribution of the numeric variables. As a wise person may – or may not – have stated: *gotta know thy data!*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform the PROC MEANS equivalent in Python\n",
    "summary = cps_df.describe()\n",
    "summary = summary.round(2)  # Set the decimal places to 2\n",
    "\n",
    "# Print the summary statistics\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see from *Unemp* and *in_LF* above that we have a good range of values - and that the average (unweighted) unemployment rate over this period is 18% and the average (unweighted) labor force participation rate is 62%. Ideally, we'd want to weight these values statistically to get a true U.S. average - but this is a fine sanity check for now. The primary takeaway is that the data appear valid and - fun fact - there are no missing values.\n",
    "\n",
    "How do we know the latter?  Well, the count is the same across the four variables - and it also matches the number of variables in the data set. (See log above.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understand the Distribution of Select Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data appear to be valid and clean.  What a great way to start as an analyst... because - truth be told - this never happens.\n",
    "\n",
    "Our next task is to examine the distribution of the categorical variables, via both tables and charts.  For example, it would be useful to know how *Education*, *Child Status*, and so on, are distributed in the data.\n",
    "\n",
    "Let's use some classic Python tools to do exactly that.  We'll keep the analysis unweighted, which still allows us to examine the relative proportion of each variable in the analysis. This is a perfectly fine test for this stage of the analysis. :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set the title for the frequency analysis\n",
    "title = \"Frequencies for Categorical Variables\"\n",
    "\n",
    "# Perform frequency analysis\n",
    "categorical_variables = ['Race_Ethnic', 'EDUC_LTD', 'Child_Status', 'Unemp', 'in_LF']\n",
    "for variable in categorical_variables:\n",
    "    freq_table = cps_df[variable].value_counts().reset_index()\n",
    "    freq_table.columns = ['Value', 'Frequency']\n",
    "    \n",
    "    # Print the frequency table\n",
    "    print(f\"\\n{title}\")\n",
    "    print(f\"\\n{variable}\")\n",
    "    print(freq_table)\n",
    "    \n",
    "    # Create frequency plot\n",
    "    sns.countplot(data=cps_df, x=variable)\n",
    "    plt.title(f\"{title}\\n{variable}\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do you see any interesting trends in the underlying data?  Again, these data aren't weighted, so they don't represent U.S. averages. But they're a useful check nonetheless.\n",
    "\n",
    "My interesting findings: (1) White, Non-Hispanic women are in, by far, the most states across the United States. (2) There are several states in the U.S. that we couldn't calculate estimates for individuals with less than a high school level of education, because sample sizes were either too small or not available. (3) Most women in the data set (a) had children, (b) were in the labor force, and (c) were employed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Use SQL to Collapse Data and Get U.S. Estimates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As noted above, an observation in the data is a particular demographic group residing in a particular state in a given year. Because we have the weighed value for each cell,    we can use SQL to aggregate the data to the country level and give us a more precise U.S. average over time. Ready to see the how elegantly SQL can collapse data?  Well, I am!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "results_dict = sas_session.submit(\n",
    "    \"\"\"\n",
    "       libname cps \"/workspaces/myfolder/SAS-OTJ-HHS2/SAS Data\"; \n",
    "        \n",
    "        proc sql;\n",
    "            create \ttable cps.covid_labor_supply_us as \n",
    "            select\tdistinct \n",
    "                    yearquarter , \n",
    "\n",
    "        /*******************************************************************  Labor Force Status | All  */\n",
    "                    sum( ( unemp=1 ) * WTFINL ) \t\t\t\t\t\t\t\t\t\t\t/ sum( ( in_LF=1 ) *   WTFINL )\t\t\t\t\t\t\t\t\t\t\tas UE_Women\t\t\t\tlabel=\"Unemployment Rate\"\tformat percent9.1 \t\t,\n",
    "                    sum( ( in_LF=1 ) * WTFINL ) \t\t\t\t\t\t\t\t\t\t\t/ sum(  \t\t\t   WTFINL ) \t\t\t\t\t\t\t\t\t\tas LFP_Women\t\t\tlabel=\"LFP Rate\"\t\t\tformat percent9.1 \t\t,\n",
    "\n",
    "\n",
    "        /*******************************************************************  Labor Force Status | By Race  */\n",
    "\n",
    "                    /*******************************************************  Unemployment */\n",
    "                    sum( ( race_ethnic=\"Black, Non-Hispanic\" ) * ( unemp=1 ) * WTFINL ) \t/ sum( ( race_ethnic=\"Black, Non-Hispanic\" ) * ( in_LF=1 ) * WTFINL ) \tas UE_BlackWomen\t\tlabel=\"Black Women\" \tformat percent9.1 \t\t,\n",
    "                    sum( ( race_ethnic=\"Hispanic\" ) * ( unemp=1 ) * WTFINL ) \t\t\t\t/ sum( ( race_ethnic=\"Hispanic\" ) * ( in_LF=1 ) * WTFINL ) \t\t\t\tas UE_HispanicWomen\t\tlabel=\"Hispanic Women\" \tformat percent9.1 \t\t,\n",
    "                    sum( ( race_ethnic=\"White, Non-Hispanic\" ) * ( unemp=1 ) * WTFINL ) \t/ sum( ( race_ethnic=\"White, Non-Hispanic\" ) * ( in_LF=1 ) * WTFINL ) \tas UE_WhiteWomen\t\tlabel=\"White Women\" \tformat percent9.1 \t\t,\n",
    "                    sum( ( race_ethnic=\"All Other\" ) * ( unemp=1 ) * WTFINL ) \t\t\t\t/ sum( ( race_ethnic=\"All Other\" ) * ( in_LF=1 ) * WTFINL ) \t\t\tas UE_OtherWomen\t\tlabel=\"All Other Women\" format percent9.1 \t\t,\n",
    "\n",
    "                    /*******************************************************  LFP */\n",
    "                    sum( ( race_ethnic=\"Black, Non-Hispanic\" ) * ( in_LF=1 ) * WTFINL ) \t/ sum( ( race_ethnic=\"Black, Non-Hispanic\" ) * WTFINL )\t\t\t\t\tas LFP_BlackWomen\t\tlabel=\"Black Women\" \tformat percent9.1 \t\t,\n",
    "                    sum( ( race_ethnic=\"Hispanic\" ) * ( in_LF=1 ) * WTFINL ) \t\t\t\t/ sum( ( race_ethnic=\"Hispanic\" ) * WTFINL ) \t\t\t\t\t\t\tas LFP_HispanicWomen\tlabel=\"Hispanic Women\" \tformat percent9.1 \t\t,\n",
    "                    sum( ( race_ethnic=\"White, Non-Hispanic\" ) * ( in_LF=1 ) * WTFINL ) \t/ sum( ( race_ethnic=\"White, Non-Hispanic\" ) * WTFINL ) \t\t\t\tas LFP_WhiteWomen\t\tlabel=\"White Women\" \tformat percent9.1 \t\t,\n",
    "                    sum( ( race_ethnic=\"All Other\" ) * ( in_LF=1 ) * WTFINL ) \t\t\t\t/ sum( ( race_ethnic=\"All Other\" ) * WTFINL )\t\t\t\t\t\t\tas LFP_OtherWomen\t\tlabel=\"All Other Women\" format percent9.1 \t\t,\n",
    "\n",
    "\n",
    "        /*******************************************************************  Labor Force Status | By Education  */\n",
    "\n",
    "                    /*******************************************************  Unemployment */\n",
    "                    sum( ( educ_ltd=\"< HS\" ) * ( unemp=1 ) * WTFINL ) \t\t\t\t\t\t/ sum( ( educ_ltd=\"< HS\" ) * ( in_LF=1 ) * WTFINL )\t\t\t\t\t\tas UE_Women_LTHS\t\tlabel=\"EDUC < HS\" \t\tformat percent9.1 \t\t,\n",
    "                    sum( ( educ_ltd=\"High School Diploma\" ) * ( unemp=1 ) * WTFINL ) \t\t/ sum( ( educ_ltd=\"High School Diploma\" ) * ( in_LF=1 ) * WTFINL ) \t\tas UE_Women_HS\t\t\tlabel=\"EDUC = HS\" \t\tformat percent9.1 \t\t,\n",
    "                    sum( ( educ_ltd=\"Some College\" ) * ( unemp=1 ) * WTFINL ) \t\t\t\t/ sum( ( educ_ltd=\"Some College\" ) * ( in_LF=1 ) * WTFINL ) \t\t\tas UE_Women_SCollege\tlabel=\"Some College\"\tformat percent9.1 \t\t,\n",
    "                    sum( ( educ_ltd=\"College +\" ) * ( unemp=1 ) * WTFINL ) \t\t\t\t\t/ sum( ( educ_ltd=\"College +\" ) * ( in_LF=1 ) * WTFINL ) \t\t\t\tas UE_Women_CollegeP\tlabel=\"College +\" \t\tformat percent9.1 \t\t,\n",
    "\n",
    "                    /*******************************************************  LFP */\n",
    "                    sum( ( educ_ltd=\"< HS\" ) * ( in_LF=1 ) * WTFINL ) \t\t\t\t\t\t/ sum( ( educ_ltd=\"< HS\" ) * WTFINL ) \t\t\t\t\t\t\t\t\tas LFP_Women_LTHS\t\tlabel=\"EDUC < HS\" \t\tformat percent9.1 \t\t,\n",
    "                    sum( ( educ_ltd=\"High School Diploma\" ) * ( in_LF=1 ) * WTFINL ) \t\t/ sum( ( educ_ltd=\"High School Diploma\" ) * WTFINL ) \t\t\t\t\tas LFP_Women_HS\t\t\tlabel=\"EDUC = HS\" \t\tformat percent9.1 \t\t,\n",
    "                    sum( ( educ_ltd=\"Some College\" ) * ( in_LF=1 ) * WTFINL ) \t\t\t\t/ sum( ( educ_ltd=\"Some College\" ) * WTFINL ) \t\t\t\t\t\t\tas LFP_Women_SCollege\tlabel=\"Some College\" \tformat percent9.1 \t\t,\n",
    "                    sum( ( educ_ltd=\"College +\" ) * ( in_LF=1 ) * WTFINL ) \t\t\t\t\t/ sum( ( educ_ltd=\"College +\" ) * WTFINL ) \t\t\t\t\t\t\t\tas LFP_Women_CollegeP\tlabel=\"College +\" \t\tformat percent9.1 \t\t,\n",
    "\n",
    "\n",
    "        /*******************************************************************  Labor Force Status | By Child Status  */\n",
    "\n",
    "                    /*******************************************************  Unemployment */\n",
    "                    sum( ( child_status=\"No Children\" ) * ( unemp=1 ) * WTFINL ) \t\t\t/ sum( ( child_status=\"No Children\" ) * ( in_LF=1 ) * WTFINL )\t\t\tas UE_Women_NoKids\t\tlabel=\"No Children\" \tformat percent9.1 \t\t,\n",
    "                    sum( ( child_status=\"Older Children\" ) * ( unemp=1 ) * WTFINL ) \t\t/ sum( ( child_status=\"Older Children\" ) * ( in_LF=1 ) * WTFINL ) \t\tas UE_Women_OlderKids\tlabel=\"Older Children\" \tformat percent9.1 \t\t,\n",
    "                    sum( ( child_status=\"Child < 5\" ) * ( unemp=1 ) * WTFINL ) \t\t\t\t/ sum( ( child_status=\"Child < 5\" ) * ( in_LF=1 ) * WTFINL ) \t\t\tas UE_Women_YoungKids\tlabel=\"Young Children\"\tformat percent9.1 \t\t,\n",
    "\n",
    "                    /*******************************************************  LFP */\n",
    "                    sum( ( child_status=\"No Children\" ) * ( in_LF=1 ) * WTFINL ) \t\t\t/ sum( ( child_status=\"No Children\" ) * WTFINL ) \t\t\t\t\t\tas LFP_Women_NoKids\t\tlabel=\"No Children\" \tformat percent9.1 \t\t,\n",
    "                    sum( ( child_status=\"Older Children\" ) * ( in_LF=1 ) * WTFINL ) \t\t/ sum( ( child_status=\"Older Children\" ) * WTFINL ) \t\t\t\t\tas LFP_Women_OlderKids\tlabel=\"Older Children\" \tformat percent9.1 \t\t,\n",
    "                    sum( ( child_status=\"Child < 5\" ) * ( in_LF=1 ) * WTFINL ) \t\t\t\t/ sum( ( child_status=\"Child < 5\" ) * WTFINL ) \t\t\t\t\t\t\tas LFP_Women_YoungKids\tlabel=\"Young Children\"\tformat percent9.1 \t\t\n",
    "\n",
    "\n",
    "            from \tcps.cps_2015_2023\n",
    "            group\tby 1 \n",
    "            order\tby 1 ;\n",
    "        quit;\n",
    "\n",
    "    \"\"\", \n",
    ")\n",
    "    \n",
    "covid_labor_supply_us_df = sas_session.sd2df(table=\"covid_labor_supply_us\",libref=\"cps\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lots of good stuff in that statement above. Note that we're collapsing to the yearquarter level - and incorporating the sample weight (that is, WTFINL) in the analysis. Notice how succinct that code is - which would take many traditional Python commands or SAS statements if we went that route instead. <cr>\n",
    "    \n",
    "Let's now summarize the data one more time, to ensure that we aggregated the data properly. One bad sign would be to see unemployment rate and labor force participation rates either less than 0 or greater than 1.  Because those things shouldn't happen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sanity Check: Do the Data Make Sense?\n",
    "Let's print the data just to ensure that our data seem reasonable. The Python code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Print the Data\n",
    "covid_labor_supply_us_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's further probe the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summarize the Data\n",
    "summary_us = covid_labor_supply_us_df.describe()\n",
    "summary_us = summary_us.round(3)  # Set the decimal places to 3\n",
    "\n",
    "# Print the summary statistics\n",
    "print(summary_us)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do you see?  My musings: (1) there are 36 observations in the aggregated data set.  The math: 9 years * 4 quarters.  (2) The (weighted) average unemployment rate is 0.042 - or 4.2%.  This value is much lower than the 0.18 (18%) value reported above - which shows that weighting is consequential and likely higher in the smaller states. (3) We see that, in general, the unemployment rate decreases, and the labor force participation rate increases, as the level of education increases. All these nuggets make sense!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1 Recap\n",
    "\n",
    "Our project is off to a great start.  We've imported the data, performed some preliminary data analysis using Python code, and collapsed the data using SQL.  But, we really haven't answered any of the interesting questions for HHS Leadership: namely (1) what are the UE and LFP trends over time and (2) are the levels back to \"normal\" after a COVID adjustment period? \n",
    "\n",
    "We'll tackle these questions better in Parts 2 and 3 of this analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# End the SAS session in VS Code\n",
    "sas_session.endsas()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
