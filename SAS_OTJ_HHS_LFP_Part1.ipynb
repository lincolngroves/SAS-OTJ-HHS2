{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# SAS On-the-Job | Part 1 \n",
    "## Role: Data Analyst within the Department of Health and Human Services"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Starting with a Story\n",
    "The coronavirus outbreak, or COVID-19, simultaneously created a health and economic crisis in the United States. As local regions locked down to slow the spread of the disease, millions of jobs were lost across the United States. Most schools also shuttered their doors and switched to remote learning. This meant that many working parents who retained their jobs found themselves as both remote workers and teacher’s assistants during the lockdowns. <br>\n",
    "\n",
    "The impacts of COVID-19 have been very uneven, especially for mothers. A focus of public policy for decades has been to find a way to get mothers back into the labor force, while supporting them as they balance work and home responsibilities. In this SAS On-the-Job activity, we explore the pre- and post-COVID-19 trends in unemployment (UE) and labor force participation (LFP) rates among U.S. women aged 25 to 54. <br>\n",
    "\n",
    "#### Job Setting \n",
    "You are a new *Public Policy Analyst* within the Department of Health and Human Services (HHS). You are placed into a department interested in public policies that promote labor force participation (LFP) by women – particularly low-income and less-educated women. Labor force participation simply means that women are engaged in the labor market - either they have a job or are looking for one.  We'll also explore the more familiar unemployment rate, which captures women who would like a job, but don't currently have one.<br>\n",
    "\n",
    "During the height of the pandemic, a host of media articles highlighted the disproportionate impact of COVID-19 on women’s employment.  HHS leadership is interested in knowing whether these impacts are still being felt today - years after the official declaration of the pandemic.  If yes, HHS leadership would consider enacting new, targeted policy response to support this potentially vulnerable group of workers. However, before considering policy responses, HHS leadership would like someone to examine whether there are lingering effects of COVID-19 on current level of labor supply by women. And this someone is you!  Drumroll, please!<br>\n",
    "\n",
    "As a new analyst, your goals are to<br>\n",
    "•\taccess the Jupyter notebooks created by your predecessor <br>\n",
    "•\tunderstand what the previous analyst did - particularly with the integration of SAS, SQL, Python, and R <br>\n",
    "•\tmodify existing code to explore (as appropriate). <br>\n",
    "\n",
    "And you'll be guided in this quest by a very helpful onboarding buddy. Which is me. Your humble narrator.\n",
    "\n",
    "Let's get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What this SAS OTJ Covers\n",
    "Beyond allowing you to play policy analyst for the day, this SAS On-the-Job (OTJ) activity provides an overview of SAS, SQL, Python, and R integration in SAS Viya Workbench.  More specifically, this SAS OTJ is an act in three parts, with the following flow: <br>\n",
    "\n",
    "(1) Start in a Python notebook - because the original coder loved Python.  Perform a preliminary analysis of the data.  Then use SQL to collapse the data - because it's a much more effective way to aggregate data than other approaches in Python.  And finally, we'll ensure that the underlying data make sense. <br>\n",
    "(2) The second notebook focuses on SAS code. We'll leverage some SAS code and macros to explore aggregated U.S. trends. We'll conclude by creating a state-level data set that we can then use to map trends in R (confession: R can produce some great maps!). <br>\n",
    "(3) The last notebook focuses on R code.  We'll plot state-level trends over time for unemployment and labor force participation. <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python and SQL Integration\n",
    "### Current Population Survey (CPS) Data Import \n",
    "\n",
    "Welcome to Day 1 as Public Policy Analyst at HHS! With access to software and your data setup, we can officially begin our analysis.  Yay!\n",
    "<br> \n",
    "\n",
    "The first thing we’ll need to do in our data adventure is get the data loaded into the environment and running. And our data come from the GitHub repo for this project:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-28T20:40:01.169277Z",
     "start_time": "2021-06-28T20:39:57.785063Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import Packages required to load data\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Setup data pathways (mirror LIBNAME in SAS... because why not?)\n",
    "cps_path = \"/workspaces/myfolder/SAS-OTJ-HHS2/SAS Data\"\n",
    "os.makedirs(cps_path, exist_ok=True)\n",
    "\n",
    "url = \"https://raw.githubusercontent.com/lincolngroves/SAS-OTJ-HHS2/main/CPS_2015_2023_ltd.csv\"\n",
    "\n",
    "cps_df = pd.read_csv(url)\n",
    "\n",
    "# Save to Disk\n",
    "cps_df.to_parquet(os.path.join(cps_path, \"cps_2015_2023.parquet\"), index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data are now loaded into your WFL environment. Game on!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examine and Explore Our Data Frame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alright - the data are now accessible in the notebook.  What's typically next as an analyst?  Well, let's examine the underlying data to become more familiar with the data and distribution of values!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# One Simple line will pull up the Python dataframe\n",
    "cps_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the data above, we can see that we'll have nine variables in our analysis.  The current unit of analysis in this data is state, year-quarter, race-ethnic, and child-status.  In simple English, this means that a row - that is, the observation - contains the unemployment rate and labor force participation rate for women of a particular race-ethnic group, education level, and child status - for a given state at a point in time.  Yup, that's a mouthful, but still very important.  Moreover, note that **Unemp** and **in_LF** are rounded in the window above - but are not in the underlying data.  Finally, **WTFINL** is a weighting variable that represents the number of women represented in that row.  So, 8709.3981 can be interpreted as ~8709 women. This variable will be very important when we aggregate the data.\n",
    "\n",
    "Finally, **FIPS** are Federal Information Processing Standards, so the **State_Fip** is just a unique variable for that state. And it will help in the upcoming merges. Finally, **State** is just, well, a U.S. state.\n",
    "\n",
    "With data now upload in Python, let's load some Python packages so that we can do some cooler things in our analysis!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-28T20:41:13.321932Z",
     "start_time": "2021-06-28T20:41:11.275031Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Classic Python packages that will make our Exploratory Data Analysis easier \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summarize the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's summarize the data in Python - so that we can better understand the distribution of the numeric variables. As a wise person may – or may not – have stated: *gotta know thy data!*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform the PROC MEANS equivalent in Python\n",
    "summary = cps_df.describe()\n",
    "summary = summary.round(2)  # Set the decimal places to 2\n",
    "\n",
    "# Print the summary statistics\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see from *Unemp* and *in_LF* above that we have a good range of values - and that the average (unweighted) unemployment rate over this period is 18% and the average (unweighted) labor force participation rate is 62%. Ideally, we'd want to weight these values statistically to get a true U.S. average - but this is a fine sanity check for now. The primary takeaway is that the data appear valid and - fun fact - there are no missing values.\n",
    "\n",
    "How do we know the latter?  Well, the count is the same across the four variables - and it also matches the number of variables in the data set. (See results above.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understand the Distribution of Select Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data appear to be valid and clean.  What a great way to start as an analyst... because - truth be told - this never happens.\n",
    "\n",
    "Our next task is to examine the distribution of the categorical variables, via both tables and charts.  For example, it would be useful to know how *Education*, *Child Status*, and so on, are distributed in the data.\n",
    "\n",
    "Let's use some classic Python tools to do exactly that.  We'll keep the analysis unweighted, which still allows us to examine the relative proportion of each variable in the analysis. This is a perfectly fine test for this stage of the analysis. :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set the title for the frequency analysis\n",
    "title = \"Frequencies for Categorical Variables\"\n",
    "\n",
    "# Perform frequency analysis\n",
    "categorical_variables = ['Race_Ethnic', 'EDUC_LTD', 'Child_Status', 'Unemp', 'in_LF']\n",
    "for variable in categorical_variables:\n",
    "    freq_table = cps_df[variable].value_counts().reset_index()\n",
    "    freq_table.columns = ['Value', 'Frequency']\n",
    "    \n",
    "    # Print the frequency table\n",
    "    print(f\"\\n{title}\")\n",
    "    print(f\"\\n{variable}\")\n",
    "    print(freq_table)\n",
    "    \n",
    "    # Create frequency plot\n",
    "    sns.countplot(data=cps_df, x=variable)\n",
    "    plt.title(f\"{title}\\n{variable}\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do you see any interesting trends in the underlying data?  Again, these data aren't weighted, so they don't represent U.S. averages. But they're a useful check nonetheless.\n",
    "\n",
    "My interesting findings: (1) White, Non-Hispanic women are in, by far, the most states across the United States. (2) There are several states in the U.S. that we couldn't calculate estimates for individuals with less than a high school level of education, because sample sizes were either too small or not available. (3) Most women in the data set (a) had children, (b) were in the labor force, and (c) were employed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Use SQL to Collapse Data and Get U.S. Estimates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As noted above, an observation in the data is a particular demographic group residing in a particular state in a given year. Because we have the weighed value for each cell,    we can use SQL to aggregate the data to the country level and give us a more precise U.S. average over time. Ready to see the how elegantly SQL can collapse data?  Well, I am!\n",
    "\n",
    "Our first step is to ensure that DuckDB is installed as the SQL engine:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  First - let's install DuckDB so we can use it as the SQL engine\n",
    "\n",
    "!pip install duckdb\n",
    "\n",
    "import duckdb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let’s register the data as a DuckDB table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register your CPS DataFrame as a DuckDB table\n",
    "con = duckdb.connect()\n",
    "con.register(\"cps_2015_2023\", cps_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally – it’s SQL code time!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "query = r\"\"\"\n",
    "SELECT\n",
    "    yearquarter,\n",
    "\n",
    "    -- Labor Force Status | All\n",
    "    SUM(CASE WHEN unemp = 1 AND in_LF = 1 THEN WTFINL ELSE 0 END)                                               / NULLIF(SUM(CASE WHEN in_LF = 1 THEN WTFINL ELSE 0 END), 0) AS ue_women,\n",
    "    SUM(CASE WHEN in_LF = 1 THEN WTFINL ELSE 0 END)                                                             / NULLIF(SUM(WTFINL), 0) AS lfp_women,\n",
    "\n",
    "    -- By Race | Unemployment\n",
    "    SUM(CASE WHEN race_ethnic = 'Black, Non-Hispanic' AND unemp = 1 AND in_LF = 1 THEN WTFINL ELSE 0 END)       / NULLIF(SUM(CASE WHEN race_ethnic = 'Black, Non-Hispanic' AND in_LF = 1 THEN WTFINL ELSE 0 END), 0) AS ue_blackwomen,\n",
    "    SUM(CASE WHEN race_ethnic = 'Hispanic' AND unemp = 1 AND in_LF = 1 THEN WTFINL ELSE 0 END)                  / NULLIF(SUM(CASE WHEN race_ethnic = 'Hispanic' AND in_LF = 1 THEN WTFINL ELSE 0 END), 0) AS ue_hispanicwomen,\n",
    "    SUM(CASE WHEN race_ethnic = 'White, Non-Hispanic' AND unemp = 1 AND in_LF = 1 THEN WTFINL ELSE 0 END)       / NULLIF(SUM(CASE WHEN race_ethnic = 'White, Non-Hispanic' AND in_LF = 1 THEN WTFINL ELSE 0 END), 0) AS ue_whitewomen,\n",
    "    SUM(CASE WHEN race_ethnic = 'All Other' AND unemp = 1 AND in_LF = 1 THEN WTFINL ELSE 0 END)                 / NULLIF(SUM(CASE WHEN race_ethnic = 'All Other' AND in_LF = 1 THEN WTFINL ELSE 0 END), 0) AS ue_otherwomen,\n",
    "\n",
    "    -- By Race | LFP\n",
    "    SUM(CASE WHEN race_ethnic = 'Black, Non-Hispanic' AND in_LF = 1 THEN WTFINL ELSE 0 END)                     / NULLIF(SUM(CASE WHEN race_ethnic = 'Black, Non-Hispanic' THEN WTFINL ELSE 0 END), 0) AS lfp_blackwomen,\n",
    "    SUM(CASE WHEN race_ethnic = 'Hispanic' AND in_LF = 1 THEN WTFINL ELSE 0 END)                                / NULLIF(SUM(CASE WHEN race_ethnic = 'Hispanic' THEN WTFINL ELSE 0 END), 0) AS lfp_hispanicwomen,\n",
    "    SUM(CASE WHEN race_ethnic = 'White, Non-Hispanic' AND in_LF = 1 THEN WTFINL ELSE 0 END)                     / NULLIF(SUM(CASE WHEN race_ethnic = 'White, Non-Hispanic' THEN WTFINL ELSE 0 END), 0) AS lfp_whitewomen,\n",
    "    SUM(CASE WHEN race_ethnic = 'All Other' AND in_LF = 1 THEN WTFINL ELSE 0 END)                               / NULLIF(SUM(CASE WHEN race_ethnic = 'All Other' THEN WTFINL ELSE 0 END), 0) AS lfp_otherwomen,\n",
    "\n",
    "    -- By Education | Unemployment\n",
    "    SUM(CASE WHEN educ_ltd = '< HS' AND unemp = 1 AND in_LF = 1 THEN WTFINL ELSE 0 END)                         / NULLIF(SUM(CASE WHEN educ_ltd = '< HS' AND in_LF = 1 THEN WTFINL ELSE 0 END), 0) AS ue_women_lths,\n",
    "    SUM(CASE WHEN educ_ltd = 'High School Diploma' AND unemp = 1 AND in_LF = 1 THEN WTFINL ELSE 0 END)          / NULLIF(SUM(CASE WHEN educ_ltd = 'High School Diploma' AND in_LF = 1 THEN WTFINL ELSE 0 END), 0) AS ue_women_hs,\n",
    "    SUM(CASE WHEN educ_ltd = 'Some College' AND unemp = 1 AND in_LF = 1 THEN WTFINL ELSE 0 END)                 / NULLIF(SUM(CASE WHEN educ_ltd = 'Some College' AND in_LF = 1 THEN WTFINL ELSE 0 END), 0) AS ue_women_scollege,\n",
    "    SUM(CASE WHEN educ_ltd = 'College +' AND unemp = 1 AND in_LF = 1 THEN WTFINL ELSE 0 END)                    / NULLIF(SUM(CASE WHEN educ_ltd = 'College +' AND in_LF = 1 THEN WTFINL ELSE 0 END), 0) AS ue_women_collegep,\n",
    "\n",
    "    -- By Education | LFP\n",
    "    SUM(CASE WHEN educ_ltd = '< HS' AND in_LF = 1 THEN WTFINL ELSE 0 END)                                       / NULLIF(SUM(CASE WHEN educ_ltd = '< HS' THEN WTFINL ELSE 0 END), 0) AS lfp_women_lths,\n",
    "    SUM(CASE WHEN educ_ltd = 'High School Diploma' AND in_LF = 1 THEN WTFINL ELSE 0 END)                        / NULLIF(SUM(CASE WHEN educ_ltd = 'High School Diploma' THEN WTFINL ELSE 0 END), 0) AS lfp_women_hs,\n",
    "    SUM(CASE WHEN educ_ltd = 'Some College' AND in_LF = 1 THEN WTFINL ELSE 0 END)                               / NULLIF(SUM(CASE WHEN educ_ltd = 'Some College' THEN WTFINL ELSE 0 END), 0) AS lfp_women_scollege,\n",
    "    SUM(CASE WHEN educ_ltd = 'College +' AND in_LF = 1 THEN WTFINL ELSE 0 END)                                  / NULLIF(SUM(CASE WHEN educ_ltd = 'College +' THEN WTFINL ELSE 0 END), 0) AS lfp_women_collegep,\n",
    "\n",
    "    -- By Child Status | Unemployment\n",
    "    SUM(CASE WHEN child_status = 'No Children' AND unemp = 1 AND in_LF = 1 THEN WTFINL ELSE 0 END)              / NULLIF(SUM(CASE WHEN child_status = 'No Children' AND in_LF = 1 THEN WTFINL ELSE 0 END), 0) AS ue_women_nokids,\n",
    "    SUM(CASE WHEN child_status = 'Older Children' AND unemp = 1 AND in_LF = 1 THEN WTFINL ELSE 0 END)           / NULLIF(SUM(CASE WHEN child_status = 'Older Children' AND in_LF = 1 THEN WTFINL ELSE 0 END), 0) AS ue_women_olderkids,\n",
    "    SUM(CASE WHEN child_status = 'Child < 5' AND unemp = 1 AND in_LF = 1 THEN WTFINL ELSE 0 END)                / NULLIF(SUM(CASE WHEN child_status = 'Child < 5' AND in_LF = 1 THEN WTFINL ELSE 0 END), 0) AS ue_women_youngkids,\n",
    "\n",
    "    -- By Child Status | LFP\n",
    "    SUM(CASE WHEN child_status = 'No Children' AND in_LF = 1 THEN WTFINL ELSE 0 END)                            / NULLIF(SUM(CASE WHEN child_status = 'No Children' THEN WTFINL ELSE 0 END), 0) AS lfp_women_nokids,\n",
    "    SUM(CASE WHEN child_status = 'Older Children' AND in_LF = 1 THEN WTFINL ELSE 0 END)                         / NULLIF(SUM(CASE WHEN child_status = 'Older Children' THEN WTFINL ELSE 0 END), 0) AS lfp_women_olderkids,\n",
    "    SUM(CASE WHEN child_status = 'Child < 5' AND in_LF = 1 THEN WTFINL ELSE 0 END)                              / NULLIF(SUM(CASE WHEN child_status = 'Child < 5' THEN WTFINL ELSE 0 END), 0) AS lfp_women_youngkids\n",
    "\n",
    "FROM cps_2015_2023\n",
    "GROUP BY yearquarter\n",
    "ORDER BY yearquarter\n",
    ";\n",
    "\"\"\"\n",
    "\n",
    "covid_labor_supply_us_df = con.execute(query).df()\n",
    "covid_labor_supply_us_df.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lots of good stuff in that statement above. Note that we're collapsing to the yearquarter level - and incorporating the sample weight (that is, WTFINL) in the analysis. Notice how succinct that code is - which would take many traditional Python commands if we went that route instead. <cr>\n",
    "    \n",
    "Let's now summarize the data one more time, to ensure that we aggregated the data properly. One bad sign would be to see unemployment rate and labor force participation rates either less than 0 or greater than 1.  Because those things shouldn't happen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sanity Check: Do the Data Make Sense?\n",
    "Let's print the data just to ensure that our data seem reasonable. The Python code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Print the Data\n",
    "covid_labor_supply_us_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's further probe the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summarize the Data\n",
    "summary_us = covid_labor_supply_us_df.describe()\n",
    "summary_us = summary_us.round(3)  # Set the decimal places to 3\n",
    "\n",
    "# Print the summary statistics\n",
    "print(summary_us)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do you see?  My musings: (1) there are 36 observations in the aggregated data set.  The math: 9 years * 4 quarters.  (2) The (weighted) average unemployment rate is 0.042 - or 4.2%.  This value is much lower than the 0.18 (18%) value reported above - which shows that weighting is consequential and likely higher in the smaller states. (3) We see that, in general, the unemployment rate decreases, and the labor force participation rate increases, as the level of education increases. All these nuggets make sense!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1 Recap\n",
    "\n",
    "Our project is off to a great start.  We've imported the data, performed some preliminary data analysis using Python code, and collapsed the data using SQL.  But, we really haven't answered any of the interesting questions for HHS Leadership: namely (1) what are the UE and LFP trends over time and (2) are the levels back to \"normal\" after a COVID adjustment period? \n",
    "\n",
    "We'll tackle these questions better in Parts 2 and 3 of this analysis."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
